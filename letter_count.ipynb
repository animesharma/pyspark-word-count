{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4121fc-ebb7-42a0-860d-ddae71ec4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d295e6-dcf0-402a-a000-650598b00b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by', 'William Shakespeare', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or', 're-use it under the terms of the Project Gutenberg License included', 'with this eBook or online at www.gutenberg.org', '', '** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **', '**     Please follow the copyright guidelines in this file.     **']\n"
     ]
    }
   ],
   "source": [
    "# Create PySpark Session\n",
    "spark = SparkSession.builder.appName(\"LetterCount\").getOrCreate()\n",
    "# Read input file and store it as a Resilient Distributed Dataset\n",
    "text = spark.sparkContext.textFile(\"./input.txt\")\n",
    "# Print first 10 lines\n",
    "print(text.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7d4d19-f52c-4047-81e2-8bd366c4894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'p', 'g', 'e', 'o', 't', 'c', 'w', 'o', 'w', 's', 'b', 'w', 's', 't', 'e', 'i', 'f', 't', 'u', 'o', 'a', 'a', 'a', 'n']\n"
     ]
    }
   ],
   "source": [
    "def splitter(line):\n",
    "    # Remove any non-words\n",
    "    line = re.sub(r\"^\\W+|\\W+$\", \"\", line)\n",
    "    # Split lines into separate words and convert all words to lowercase\n",
    "    words = map(str.lower, re.split(r\"\\W+\", line))\n",
    "    # Replace non-letter characters with empty strings\n",
    "    words = [re.sub(r'[^a-z]+', '', word) for word in words]\n",
    "    # Remove all empty strings\n",
    "    words = list(filter(None, words))\n",
    "    # Return a list of the first character of each words\n",
    "    return [word[0] for word in words]\n",
    "\n",
    "# Create an RDD of first letter of each word in the input file\n",
    "letters = text.flatMap(splitter)\n",
    "print(letters.take(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4328d87d-4542-464b-9c91-2616aeb44f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 1), ('p', 1), ('g', 1), ('e', 1), ('o', 1), ('t', 1), ('c', 1), ('w', 1), ('o', 1), ('w', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Map Step\n",
    "\n",
    "# Assign a count of 1 to each letter\n",
    "mapped_letters = letters.map(lambda x: (x,1))\n",
    "print(mapped_letters.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861f5f9a-f8e8-4c17-bff4-3f574201ffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('p', 28059), ('g', 21167), ('c', 34983), ('s', 75226), ('b', 46001), ('i', 62420), ('r', 15234), ('y', 25926), ('l', 32389), ('d', 39173)]\n"
     ]
    }
   ],
   "source": [
    "# Reduce Step\n",
    "from operator import add\n",
    "\n",
    "# Aggregate the total count of each letter\n",
    "counts = mapped_letters.reduceByKey(add)\n",
    "print(counts.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89a73b7-139f-40a2-844b-1455b223f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store letter_counts as a dictionary with starting letter as key and count as value\n",
    "letter_counts = {item[0] : item[1] for item in counts.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d306b15a-b32f-4bf2-a06d-97dfb7983aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result as JSON\n",
    "import json\n",
    "with open(\"./output/letter_count.json\", \"w\") as outfile:\n",
    "    json.dump(letter_counts, outfile, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "* Spark 3 in Python 3.8",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
